package services

import (
	"context"
	"encoding/json"
	"fmt"
	"log"
	"strings"
	"sync"
	"time"

	"github.com/contextkeeper/service/internal/llm"
	"github.com/contextkeeper/service/internal/models"
)

// RealLLMService çœŸå®çš„LLMæœåŠ¡å®ç°
// åŸºäºç°æœ‰çš„LLMå®¢æˆ·ç«¯åŸºç¡€è®¾æ–½ï¼Œæä¾›æ™ºèƒ½çš„æ„å›¾åˆ†æå’Œä¸Šä¸‹æ–‡åˆæˆåŠŸèƒ½
type RealLLMService struct {
	// LLMå®¢æˆ·ç«¯
	llmClient llm.LLMClient

	// é…ç½®
	provider    string
	model       string
	maxTokens   int
	temperature float64

	// å¹¶å‘å®‰å…¨
	mutex sync.RWMutex

	// æ€§èƒ½ç›‘æ§
	requestCount    int64
	successCount    int64
	errorCount      int64
	totalLatency    time.Duration
	lastRequestTime time.Time
}

// NewRealLLMService åˆ›å»ºçœŸå®çš„LLMæœåŠ¡
func NewRealLLMService(provider, model, apiKey string) (*RealLLMService, error) {
	log.Printf("ğŸ¤– [çœŸå®LLM] å¼€å§‹åˆå§‹åŒ–çœŸå®LLMæœåŠ¡ï¼Œæä¾›å•†: %s, æ¨¡å‹: %s", provider, model)

	// åˆ›å»ºLLMé…ç½®
	config := &llm.LLMConfig{
		Provider:   llm.LLMProvider(provider),
		APIKey:     apiKey,
		Model:      model,
		MaxRetries: 3,
		Timeout:    120 * time.Second,
		RateLimit:  60, // æ¯åˆ†é’Ÿ60æ¬¡è¯·æ±‚
	}

	// ğŸ†• è®¾ç½®æœ¬åœ°æ¨¡å‹çš„ç‰¹æ®Šé…ç½®
	if provider == "ollama_local" {
		config.BaseURL = "http://localhost:11434"
		config.RateLimit = 0              // æœ¬åœ°æ¨¡å‹æ— é™æµé™åˆ¶
		config.Timeout = 60 * time.Second // æœ¬åœ°æ¨¡å‹æ›´å¿«
		config.APIKey = ""                // æœ¬åœ°æ¨¡å‹ä¸éœ€è¦APIå¯†é’¥
	}

	// è®¾ç½®å…¨å±€é…ç½®
	llm.SetGlobalConfig(llm.LLMProvider(provider), config)

	// åˆ›å»ºLLMå®¢æˆ·ç«¯
	client, err := llm.CreateGlobalClient(llm.LLMProvider(provider))
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºLLMå®¢æˆ·ç«¯å¤±è´¥: %w", err)
	}

	// éªŒè¯å®¢æˆ·ç«¯è¿æ¥
	ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
	defer cancel()

	if err := client.HealthCheck(ctx); err != nil {
		log.Printf("âš ï¸ [çœŸå®LLM] LLMå®¢æˆ·ç«¯å¥åº·æ£€æŸ¥å¤±è´¥: %vï¼Œä½†ç»§ç»­åˆå§‹åŒ–", err)
	} else {
		log.Printf("âœ… [çœŸå®LLM] LLMå®¢æˆ·ç«¯å¥åº·æ£€æŸ¥é€šè¿‡")
	}

	service := &RealLLMService{
		llmClient:   client,
		provider:    provider,
		model:       model,
		maxTokens:   4000,
		temperature: 0.7,
	}

	log.Printf("âœ… [çœŸå®LLM] çœŸå®LLMæœåŠ¡åˆå§‹åŒ–å®Œæˆï¼Œæä¾›å•†: %s, æ¨¡å‹: %s", provider, model)
	return service, nil
}

// AnalyzeUserIntent åˆ†æç”¨æˆ·æ„å›¾
func (rls *RealLLMService) AnalyzeUserIntent(userQuery string) (*models.IntentAnalysisResult, error) {
	startTime := time.Now()
	log.Printf("ğŸ¯ [çœŸå®LLM] å¼€å§‹åˆ†æç”¨æˆ·æ„å›¾ï¼ŒæŸ¥è¯¢é•¿åº¦: %d", len(userQuery))

	rls.mutex.Lock()
	rls.requestCount++
	rls.lastRequestTime = startTime
	rls.mutex.Unlock()

	// æ„å»ºæ„å›¾åˆ†æçš„ç³»ç»Ÿæç¤ºè¯
	systemPrompt := `ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ„å›¾åˆ†æä¸“å®¶ã€‚è¯·åˆ†æç”¨æˆ·æŸ¥è¯¢çš„æ ¸å¿ƒæ„å›¾ã€é¢†åŸŸä¸Šä¸‹æ–‡å’Œåº”ç”¨åœºæ™¯ã€‚

è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
{
  "core_intent": "æ ¸å¿ƒæ„å›¾ï¼ˆå¦‚ï¼šå¼€å‘ã€ä¿®å¤ã€ä¼˜åŒ–ã€åˆ†æã€è®¾è®¡ã€æµ‹è¯•ã€éƒ¨ç½²ã€å­¦ä¹ ã€æŸ¥è¯¢ã€é…ç½®ï¼‰",
  "domain_context": "æŠ€æœ¯é¢†åŸŸï¼ˆå¦‚ï¼šGoè¯­è¨€ã€å‰ç«¯å¼€å‘ã€åç«¯å¼€å‘ã€æ•°æ®åº“ã€ç³»ç»Ÿæ¶æ„ã€DevOpsã€æœºå™¨å­¦ä¹ ã€ç½‘ç»œå®‰å…¨ã€äº‘è®¡ç®—ã€ç§»åŠ¨å¼€å‘ï¼‰",
  "scenario": "åº”ç”¨åœºæ™¯ï¼ˆå¦‚ï¼šé¡¹ç›®å¼€å‘ã€é—®é¢˜æ’æŸ¥ã€æ€§èƒ½ä¼˜åŒ–ã€ç³»ç»Ÿç»´æŠ¤ã€å­¦ä¹ ç ”ç©¶ã€ä»£ç å®¡æŸ¥ã€æµ‹è¯•éªŒè¯ã€æ¶æ„è®¾è®¡ï¼‰",
  "multi_intents": ["å¦‚æœå­˜åœ¨å¤šä¸ªæ„å›¾ï¼Œåˆ—å‡ºæ‰€æœ‰æ£€æµ‹åˆ°çš„æ„å›¾"],
  "confidence": 0.95
}

è¯·ç¡®ä¿è¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼ã€‚`

	userPrompt := fmt.Sprintf("è¯·åˆ†æä»¥ä¸‹ç”¨æˆ·æŸ¥è¯¢çš„æ„å›¾ï¼š\n\n%s", userQuery)

	// è°ƒç”¨LLM
	ctx, cancel := context.WithTimeout(context.Background(), 60*time.Second)
	defer cancel()

	req := &llm.LLMRequest{
		SystemPrompt: systemPrompt,
		Prompt:       userPrompt,
		MaxTokens:    1000,
		Temperature:  0.3, // è¾ƒä½æ¸©åº¦ç¡®ä¿ç¨³å®šè¾“å‡º
		Format:       "json",
	}

	resp, err := rls.llmClient.Complete(ctx, req)
	if err != nil {
		rls.recordError()
		log.Printf("âŒ [çœŸå®LLM] æ„å›¾åˆ†æLLMè°ƒç”¨å¤±è´¥: %v", err)
		return nil, fmt.Errorf("LLMæ„å›¾åˆ†æå¤±è´¥: %w", err)
	}

	// è§£æLLMå“åº”
	result, err := rls.parseIntentAnalysisResponse(resp.Content)
	if err != nil {
		rls.recordError()
		log.Printf("âŒ [çœŸå®LLM] æ„å›¾åˆ†æå“åº”è§£æå¤±è´¥: %v", err)
		return nil, fmt.Errorf("æ„å›¾åˆ†æå“åº”è§£æå¤±è´¥: %w", err)
	}

	// è®°å½•æˆåŠŸ
	rls.recordSuccess(time.Since(startTime))

	log.Printf("âœ… [çœŸå®LLM] æ„å›¾åˆ†æå®Œæˆï¼Œæ ¸å¿ƒæ„å›¾: %s, é¢†åŸŸ: %s, åœºæ™¯: %s, è€—æ—¶: %v",
		result.CoreIntentText, result.DomainContextText, result.ScenarioText, time.Since(startTime))

	return result, nil
}

// SynthesizeAndEvaluateContext åˆæˆå’Œè¯„ä¼°ä¸Šä¸‹æ–‡
func (rls *RealLLMService) SynthesizeAndEvaluateContext(
	userQuery string,
	currentContext *models.UnifiedContextModel,
	retrievalResults *models.ParallelRetrievalResult,
	intentAnalysis *models.IntentAnalysisResult,
) (*models.ContextSynthesisResult, error) {
	startTime := time.Now()
	log.Printf("ğŸ§  [çœŸå®LLM] å¼€å§‹ä¸Šä¸‹æ–‡åˆæˆä¸è¯„ä¼°ï¼ŒæŸ¥è¯¢: %s", truncateString(userQuery, 50))

	rls.mutex.Lock()
	rls.requestCount++
	rls.lastRequestTime = startTime
	rls.mutex.Unlock()

	// æ„å»ºä¸Šä¸‹æ–‡åˆæˆçš„ç³»ç»Ÿæç¤ºè¯
	systemPrompt := rls.buildContextSynthesisPrompt()

	// æ„å»ºç”¨æˆ·æç¤ºè¯
	userPrompt := rls.buildContextSynthesisUserPrompt(userQuery, currentContext, retrievalResults, intentAnalysis)

	// è°ƒç”¨LLM
	ctx, cancel := context.WithTimeout(context.Background(), 90*time.Second)
	defer cancel()

	req := &llm.LLMRequest{
		SystemPrompt: systemPrompt,
		Prompt:       userPrompt,
		MaxTokens:    2000,
		Temperature:  0.5,
		Format:       "json",
	}

	resp, err := rls.llmClient.Complete(ctx, req)
	if err != nil {
		rls.recordError()
		log.Printf("âŒ [çœŸå®LLM] ä¸Šä¸‹æ–‡åˆæˆLLMè°ƒç”¨å¤±è´¥: %v", err)
		return nil, fmt.Errorf("LLMä¸Šä¸‹æ–‡åˆæˆå¤±è´¥: %w", err)
	}

	// è§£æLLMå“åº”
	result, err := rls.parseContextSynthesisResponse(resp.Content, currentContext, intentAnalysis)
	if err != nil {
		rls.recordError()
		log.Printf("âŒ [çœŸå®LLM] ä¸Šä¸‹æ–‡åˆæˆå“åº”è§£æå¤±è´¥: %v", err)
		return nil, fmt.Errorf("ä¸Šä¸‹æ–‡åˆæˆå“åº”è§£æå¤±è´¥: %w", err)
	}

	// è®°å½•æˆåŠŸ
	rls.recordSuccess(time.Since(startTime))

	log.Printf("âœ… [çœŸå®LLM] ä¸Šä¸‹æ–‡åˆæˆå®Œæˆï¼Œæ˜¯å¦æ›´æ–°: %t, ç½®ä¿¡åº¦: %.2f, è€—æ—¶: %v",
		result.ShouldUpdate, result.UpdateConfidence, time.Since(startTime))

	return result, nil
}

// GenerateResponse ç”Ÿæˆå“åº”ï¼ˆå®ç°LLMServiceæ¥å£ï¼‰
func (rls *RealLLMService) GenerateResponse(ctx context.Context, req *GenerateRequest) (*GenerateResponse, error) {
	startTime := time.Now()
	log.Printf("ğŸ¤– [çœŸå®LLM] GenerateResponseè°ƒç”¨å¼€å§‹")

	// è¯¦ç»†è®°å½•å…¥å‚
	log.Printf("ğŸ“¤ [LLMå…¥å‚] ==================== è¯·æ±‚è¯¦æƒ… ====================")
	log.Printf("ğŸ“¤ [LLMå…¥å‚] Prompté•¿åº¦: %då­—ç¬¦", len(req.Prompt))
	log.Printf("ğŸ“¤ [LLMå…¥å‚] MaxTokens: %d", req.MaxTokens)
	log.Printf("ğŸ“¤ [LLMå…¥å‚] Temperature: %.2f", req.Temperature)
	log.Printf("ğŸ“¤ [LLMå…¥å‚] Format: %s", req.Format)
	log.Printf("ğŸ“¤ [LLMå…¥å‚] è¶…æ—¶è®¾ç½®: %v", ctx.Value("timeout"))

	// æ˜¾ç¤ºå®Œæ•´çš„Promptå†…å®¹
	log.Printf("ğŸ“¤ [LLMå…¥å‚] å®Œæ•´Promptå†…å®¹:")
	log.Printf("=== PROMPTå¼€å§‹ ===")
	log.Printf("%s", req.Prompt)
	log.Printf("=== PROMPTç»“æŸ ===")

	rls.mutex.Lock()
	rls.requestCount++
	requestID := rls.requestCount
	rls.mutex.Unlock()

	log.Printf("ğŸ“¤ [LLMå…¥å‚] è¯·æ±‚ID: %d", requestID)

	// æ„å»ºLLMè¯·æ±‚
	llmRequest := &llm.LLMRequest{
		Prompt:      req.Prompt,
		MaxTokens:   req.MaxTokens,
		Temperature: req.Temperature,
		Format:      req.Format,
	}

	log.Printf("â³ [çœŸå®LLM] å¼€å§‹è°ƒç”¨DeepSeek APIï¼Œè¯·æ±‚ID: %d", requestID)

	// è°ƒç”¨LLMå®¢æˆ·ç«¯
	response, err := rls.llmClient.Complete(ctx, llmRequest)
	duration := time.Since(startTime)

	if err != nil {
		log.Printf("âŒ [LLMå‡ºå‚] ==================== é”™è¯¯å“åº” ====================")
		log.Printf("âŒ [LLMå‡ºå‚] è¯·æ±‚ID: %d", requestID)
		log.Printf("âŒ [LLMå‡ºå‚] é”™è¯¯ä¿¡æ¯: %v", err)
		log.Printf("âŒ [LLMå‡ºå‚] è€—æ—¶: %v", duration)
		log.Printf("âŒ [LLMå‡ºå‚] ================================================")

		rls.recordError()
		return nil, fmt.Errorf("LLMè°ƒç”¨å¤±è´¥: %w", err)
	}

	// è¯¦ç»†è®°å½•å‡ºå‚
	log.Printf("ğŸ“¥ [LLMå‡ºå‚] ==================== æˆåŠŸå“åº” ====================")
	log.Printf("ğŸ“¥ [LLMå‡ºå‚] è¯·æ±‚ID: %d", requestID)
	log.Printf("ğŸ“¥ [LLMå‡ºå‚] å“åº”é•¿åº¦: %då­—ç¬¦", len(response.Content))
	log.Printf("ğŸ“¥ [LLMå‡ºå‚] Tokenä½¿ç”¨: %d", response.TokensUsed)
	log.Printf("ğŸ“¥ [LLMå‡ºå‚] æ¨¡å‹: %s", response.Model)
	log.Printf("ğŸ“¥ [LLMå‡ºå‚] æä¾›å•†: %s", response.Provider)
	log.Printf("ğŸ“¥ [LLMå‡ºå‚] è€—æ—¶: %v", duration)
	log.Printf("ğŸ“¥ [LLMå‡ºå‚] ç”Ÿæˆé€Ÿåº¦: %.1f tokens/ç§’", float64(response.TokensUsed)/duration.Seconds())

	// æ˜¾ç¤ºå®Œæ•´çš„å“åº”å†…å®¹
	log.Printf("ğŸ“¥ [LLMå‡ºå‚] å®Œæ•´å“åº”å†…å®¹:")
	log.Printf("=== RESPONSEå¼€å§‹ ===")
	log.Printf("%s", response.Content)
	log.Printf("=== RESPONSEç»“æŸ ===")
	log.Printf("ğŸ“¥ [LLMå‡ºå‚] ================================================")

	rls.recordSuccess(duration)

	return &GenerateResponse{
		Content: response.Content,
		Usage: Usage{
			PromptTokens:     response.TokensUsed / 2, // ä¼°ç®—
			CompletionTokens: response.TokensUsed / 2, // ä¼°ç®—
			TotalTokens:      response.TokensUsed,
		},
	}, nil
}

// parseIntentAnalysisResponse è§£ææ„å›¾åˆ†æå“åº”
func (rls *RealLLMService) parseIntentAnalysisResponse(content string) (*models.IntentAnalysisResult, error) {
	// æ¸…ç†å“åº”å†…å®¹
	content = strings.TrimSpace(content)

	// å°è¯•æå–JSONéƒ¨åˆ†
	if strings.Contains(content, "```json") {
		start := strings.Index(content, "```json") + 7
		end := strings.Index(content[start:], "```")
		if end > 0 {
			content = content[start : start+end]
		}
	} else if strings.Contains(content, "```") {
		start := strings.Index(content, "```") + 3
		end := strings.Index(content[start:], "```")
		if end > 0 {
			content = content[start : start+end]
		}
	}

	content = strings.TrimSpace(content)

	// è§£æJSON
	var response struct {
		CoreIntent    string   `json:"core_intent"`
		DomainContext string   `json:"domain_context"`
		Scenario      string   `json:"scenario"`
		MultiIntents  []string `json:"multi_intents"`
		Confidence    float64  `json:"confidence"`
	}

	if err := json.Unmarshal([]byte(content), &response); err != nil {
		log.Printf("âš ï¸ [çœŸå®LLM] JSONè§£æå¤±è´¥ï¼Œå°è¯•æ–‡æœ¬è§£æ: %v", err)
		return rls.parseIntentAnalysisFromText(content)
	}

	// æ„å»ºç»“æœ
	result := &models.IntentAnalysisResult{
		CoreIntentText:       response.CoreIntent,
		DomainContextText:    response.DomainContext,
		ScenarioText:         response.Scenario,
		IntentCount:          len(response.MultiIntents),
		MultiIntentBreakdown: response.MultiIntents,
	}

	// è®¾ç½®é»˜è®¤å€¼
	if result.CoreIntentText == "" {
		result.CoreIntentText = "é€šç”¨æŸ¥è¯¢"
	}
	if result.DomainContextText == "" {
		result.DomainContextText = "é€šç”¨æŠ€æœ¯"
	}
	if result.ScenarioText == "" {
		result.ScenarioText = "æ—¥å¸¸å¼€å‘"
	}
	if len(result.MultiIntentBreakdown) == 0 {
		result.MultiIntentBreakdown = []string{result.CoreIntentText}
		result.IntentCount = 1
	}

	return result, nil
}

// parseIntentAnalysisFromText ä»æ–‡æœ¬è§£ææ„å›¾åˆ†æï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
func (rls *RealLLMService) parseIntentAnalysisFromText(content string) (*models.IntentAnalysisResult, error) {
	log.Printf("ğŸ”„ [çœŸå®LLM] ä½¿ç”¨æ–‡æœ¬è§£æå¤‡ç”¨æ–¹æ¡ˆ")

	// ç®€å•çš„æ–‡æœ¬è§£æé€»è¾‘
	contentLower := strings.ToLower(content)

	// æ„å›¾å…³é”®è¯æ˜ å°„
	intentKeywords := map[string][]string{
		"å¼€å‘": {"å¼€å‘", "å®ç°", "åˆ›å»º", "æ„å»º", "ç¼–å†™"},
		"ä¿®å¤": {"ä¿®å¤", "è§£å†³", "è°ƒè¯•", "ä¿®æ”¹", "çº æ­£"},
		"ä¼˜åŒ–": {"ä¼˜åŒ–", "æ”¹è¿›", "æå‡", "å¢å¼º", "å®Œå–„"},
		"åˆ†æ": {"åˆ†æ", "ç ”ç©¶", "è°ƒæŸ¥", "æ£€æŸ¥", "è¯„ä¼°"},
		"è®¾è®¡": {"è®¾è®¡", "è§„åˆ’", "æ¶æ„", "å»ºæ¨¡", "æ„æ€"},
		"æµ‹è¯•": {"æµ‹è¯•", "éªŒè¯", "æ£€éªŒ", "æ ¡éªŒ", "ç¡®è®¤"},
		"éƒ¨ç½²": {"éƒ¨ç½²", "å‘å¸ƒ", "ä¸Šçº¿", "å®‰è£…", "é…ç½®"},
		"å­¦ä¹ ": {"å­¦ä¹ ", "äº†è§£", "æŒæ¡", "ç†è§£", "ç ”ä¹ "},
		"æŸ¥è¯¢": {"æŸ¥è¯¢", "æœç´¢", "æŸ¥æ‰¾", "è·å–", "æ£€ç´¢"},
	}

	coreIntent := "é€šç”¨æŸ¥è¯¢"
	for intent, keywords := range intentKeywords {
		for _, keyword := range keywords {
			if strings.Contains(contentLower, keyword) {
				coreIntent = intent
				break
			}
		}
		if coreIntent != "é€šç”¨æŸ¥è¯¢" {
			break
		}
	}

	return &models.IntentAnalysisResult{
		CoreIntentText:       coreIntent,
		DomainContextText:    "é€šç”¨æŠ€æœ¯",
		ScenarioText:         "æ—¥å¸¸å¼€å‘",
		IntentCount:          1,
		MultiIntentBreakdown: []string{coreIntent},
	}, nil
}

// recordSuccess è®°å½•æˆåŠŸè¯·æ±‚
func (rls *RealLLMService) recordSuccess(latency time.Duration) {
	rls.mutex.Lock()
	defer rls.mutex.Unlock()

	rls.successCount++
	rls.totalLatency += latency
}

// recordError è®°å½•é”™è¯¯è¯·æ±‚
func (rls *RealLLMService) recordError() {
	rls.mutex.Lock()
	defer rls.mutex.Unlock()

	rls.errorCount++
}

// buildContextSynthesisPrompt æ„å»ºä¸Šä¸‹æ–‡åˆæˆçš„ç³»ç»Ÿæç¤ºè¯
func (rls *RealLLMService) buildContextSynthesisPrompt() string {
	return `ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸Šä¸‹æ–‡åˆæˆä¸è¯„ä¼°ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æç”¨æˆ·æŸ¥è¯¢ã€ç°æœ‰ä¸Šä¸‹æ–‡å’Œæ£€ç´¢ç»“æœï¼Œå†³å®šæ˜¯å¦éœ€è¦æ›´æ–°ä¸Šä¸‹æ–‡ï¼Œå¹¶æä¾›åˆæˆåçš„ä¸Šä¸‹æ–‡ã€‚

è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
{
  "should_update": true/false,
  "update_confidence": 0.85,
  "evaluation_reason": "è¯¦ç»†çš„è¯„ä¼°åŸå› ",
  "updated_context": {
    "core_concepts": ["æ ¸å¿ƒæ¦‚å¿µ1", "æ ¸å¿ƒæ¦‚å¿µ2"],
    "key_relationships": ["å…³ç³»1", "å…³ç³»2"],
    "important_details": ["é‡è¦ç»†èŠ‚1", "é‡è¦ç»†èŠ‚2"],
    "context_summary": "ä¸Šä¸‹æ–‡æ€»ç»“"
  },
  "information_sources": {
    "timeline_contribution": 0.3,
    "knowledge_contribution": 0.3,
    "vector_contribution": 0.3,
    "context_contribution": 0.1
  },
  "semantic_changes": [
    {
      "type": "addition/modification/removal",
      "description": "å˜æ›´æè¿°",
      "impact_level": "high/medium/low"
    }
  ]
}

è¯„ä¼°æ ‡å‡†ï¼š
1. å¦‚æœç”¨æˆ·æŸ¥è¯¢ä¸ç°æœ‰ä¸Šä¸‹æ–‡é«˜åº¦ç›¸å…³ä¸”æ— æ–°ä¿¡æ¯ï¼Œshould_update=false
2. å¦‚æœæ£€ç´¢åˆ°æ–°çš„ç›¸å…³ä¿¡æ¯æˆ–ç”¨æˆ·æŸ¥è¯¢å¸¦æ¥æ–°çš„ä¸Šä¸‹æ–‡ï¼Œshould_update=true
3. update_confidenceåŸºäºä¿¡æ¯çš„ç›¸å…³æ€§å’Œå¯é æ€§
4. ä¼˜å…ˆä¿ç•™é‡è¦çš„å†å²ä¸Šä¸‹æ–‡ï¼ŒåŒæ—¶èåˆæ–°ä¿¡æ¯

è¯·ç¡®ä¿è¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼ã€‚`
}

// buildContextSynthesisUserPrompt æ„å»ºä¸Šä¸‹æ–‡åˆæˆçš„ç”¨æˆ·æç¤ºè¯
func (rls *RealLLMService) buildContextSynthesisUserPrompt(
	userQuery string,
	currentContext *models.UnifiedContextModel,
	retrievalResults *models.ParallelRetrievalResult,
	intentAnalysis *models.IntentAnalysisResult,
) string {
	prompt := fmt.Sprintf("ç”¨æˆ·æŸ¥è¯¢ï¼š%s\n\n", userQuery)

	// æ·»åŠ æ„å›¾åˆ†æä¿¡æ¯
	prompt += fmt.Sprintf("æ„å›¾åˆ†æï¼š\n- æ ¸å¿ƒæ„å›¾ï¼š%s\n- æŠ€æœ¯é¢†åŸŸï¼š%s\n- åº”ç”¨åœºæ™¯ï¼š%s\n\n",
		intentAnalysis.CoreIntentText, intentAnalysis.DomainContextText, intentAnalysis.ScenarioText)

	// æ·»åŠ ç°æœ‰ä¸Šä¸‹æ–‡ä¿¡æ¯
	if currentContext != nil {
		prompt += "ç°æœ‰ä¸Šä¸‹æ–‡ï¼š\n"
		if currentContext.CurrentTopic != nil {
			prompt += fmt.Sprintf("- ä¸»è¦è¯é¢˜ï¼š%s\n", currentContext.CurrentTopic.MainTopic)
			prompt += fmt.Sprintf("- ç”¨æˆ·æ„å›¾ï¼š%s\n", currentContext.CurrentTopic.UserIntent)
			if currentContext.CurrentTopic.PrimaryPainPoint != "" {
				prompt += fmt.Sprintf("- ä¸»è¦ç—›ç‚¹ï¼š%s\n", currentContext.CurrentTopic.PrimaryPainPoint)
			}
			if len(currentContext.CurrentTopic.KeyConcepts) > 0 {
				concepts := make([]string, len(currentContext.CurrentTopic.KeyConcepts))
				for i, concept := range currentContext.CurrentTopic.KeyConcepts {
					concepts[i] = concept.ConceptName
				}
				prompt += fmt.Sprintf("- å…³é”®æ¦‚å¿µï¼š%v\n", concepts)
			}
		}
		if currentContext.Project != nil && currentContext.Project.ProjectName != "" {
			prompt += fmt.Sprintf("- é¡¹ç›®ï¼š%s\n", currentContext.Project.ProjectName)
		}
		prompt += "\n"
	} else {
		prompt += "ç°æœ‰ä¸Šä¸‹æ–‡ï¼šæ— ï¼ˆé¦–æ¬¡åˆ›å»ºï¼‰\n\n"
	}

	// æ·»åŠ æ£€ç´¢ç»“æœä¿¡æ¯
	prompt += "æ£€ç´¢ç»“æœï¼š\n"
	if retrievalResults != nil {
		prompt += fmt.Sprintf("- æ—¶é—´çº¿æ•°æ®ï¼š%dæ¡è®°å½•\n", retrievalResults.TimelineCount)
		prompt += fmt.Sprintf("- çŸ¥è¯†å›¾è°±ï¼š%dæ¡è®°å½•\n", retrievalResults.KnowledgeCount)
		prompt += fmt.Sprintf("- å‘é‡æ£€ç´¢ï¼š%dæ¡è®°å½•\n", retrievalResults.VectorCount)

		// æ·»åŠ å…·ä½“çš„æ£€ç´¢å†…å®¹ï¼ˆæˆªæ–­æ˜¾ç¤ºï¼‰
		if len(retrievalResults.TimelineResults) > 0 {
			prompt += "æ—¶é—´çº¿æ•°æ®ç¤ºä¾‹ï¼š\n"
			for i, item := range retrievalResults.TimelineResults {
				if i >= 3 { // æœ€å¤šæ˜¾ç¤º3æ¡
					break
				}
				prompt += fmt.Sprintf("  - %s\n", truncateString(fmt.Sprintf("%v", item), 100))
			}
		}

		if len(retrievalResults.KnowledgeResults) > 0 {
			prompt += "çŸ¥è¯†å›¾è°±ç¤ºä¾‹ï¼š\n"
			for i, item := range retrievalResults.KnowledgeResults {
				if i >= 3 { // æœ€å¤šæ˜¾ç¤º3æ¡
					break
				}
				prompt += fmt.Sprintf("  - %s\n", truncateString(fmt.Sprintf("%v", item), 100))
			}
		}

		if len(retrievalResults.VectorResults) > 0 {
			prompt += "å‘é‡æ£€ç´¢ç¤ºä¾‹ï¼š\n"
			for i, item := range retrievalResults.VectorResults {
				if i >= 3 { // æœ€å¤šæ˜¾ç¤º3æ¡
					break
				}
				prompt += fmt.Sprintf("  - %s\n", truncateString(fmt.Sprintf("%v", item), 100))
			}
		}
	} else {
		prompt += "- æ— æ£€ç´¢ç»“æœ\n"
	}

	prompt += "\nè¯·åŸºäºä»¥ä¸Šä¿¡æ¯è¿›è¡Œä¸Šä¸‹æ–‡åˆæˆä¸è¯„ä¼°ã€‚"

	return prompt
}

// parseContextSynthesisResponse è§£æä¸Šä¸‹æ–‡åˆæˆå“åº”
func (rls *RealLLMService) parseContextSynthesisResponse(
	content string,
	currentContext *models.UnifiedContextModel,
	intentAnalysis *models.IntentAnalysisResult,
) (*models.ContextSynthesisResult, error) {
	// æ¸…ç†å“åº”å†…å®¹
	content = strings.TrimSpace(content)

	// å°è¯•æå–JSONéƒ¨åˆ†
	if strings.Contains(content, "```json") {
		start := strings.Index(content, "```json") + 7
		end := strings.Index(content[start:], "```")
		if end > 0 {
			content = content[start : start+end]
		}
	} else if strings.Contains(content, "```") {
		start := strings.Index(content, "```") + 3
		end := strings.Index(content[start:], "```")
		if end > 0 {
			content = content[start : start+end]
		}
	}

	content = strings.TrimSpace(content)

	// è§£æJSON
	var response struct {
		ShouldUpdate     bool    `json:"should_update"`
		UpdateConfidence float64 `json:"update_confidence"`
		EvaluationReason string  `json:"evaluation_reason"`
		UpdatedContext   struct {
			CoreConcepts     []string `json:"core_concepts"`
			KeyRelationships []string `json:"key_relationships"`
			ImportantDetails []string `json:"important_details"`
			ContextSummary   string   `json:"context_summary"`
		} `json:"updated_context"`
		InformationSources struct {
			TimelineContribution  float64 `json:"timeline_contribution"`
			KnowledgeContribution float64 `json:"knowledge_contribution"`
			VectorContribution    float64 `json:"vector_contribution"`
			ContextContribution   float64 `json:"context_contribution"`
		} `json:"information_sources"`
		SemanticChanges []struct {
			Type        string `json:"type"`
			Description string `json:"description"`
			ImpactLevel string `json:"impact_level"`
		} `json:"semantic_changes"`
	}

	if err := json.Unmarshal([]byte(content), &response); err != nil {
		log.Printf("âš ï¸ [çœŸå®LLM] JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ: %v", err)
		return rls.createFallbackSynthesisResult(currentContext, intentAnalysis)
	}

	// æ„å»ºæ›´æ–°åçš„ä¸Šä¸‹æ–‡
	var updatedContext *models.UnifiedContextModel
	if response.ShouldUpdate {
		// åˆ›å»ºæ–°çš„ä¸Šä¸‹æ–‡æ¨¡å‹
		updatedContext = &models.UnifiedContextModel{
			CreatedAt: time.Now(),
			UpdatedAt: time.Now(),
		}

		// è®¾ç½®ä¼šè¯ID
		if currentContext != nil {
			updatedContext.SessionID = currentContext.SessionID
		} else {
			updatedContext.SessionID = "unknown" // è¿™åº”è¯¥ç”±è°ƒç”¨æ–¹è®¾ç½®
		}

		// è®¾ç½®å½“å‰ä¸»é¢˜ä¸Šä¸‹æ–‡
		if len(response.UpdatedContext.CoreConcepts) > 0 || response.UpdatedContext.ContextSummary != "" {
			updatedContext.CurrentTopic = &models.TopicContext{
				MainTopic:        response.UpdatedContext.ContextSummary,
				PrimaryPainPoint: "åŸºäºLLMåˆ†æçš„ä¸Šä¸‹æ–‡",
			}

			// æ·»åŠ å…³é”®æ¦‚å¿µ
			if len(response.UpdatedContext.CoreConcepts) > 0 {
				updatedContext.CurrentTopic.KeyConcepts = make([]models.ConceptInfo, len(response.UpdatedContext.CoreConcepts))
				for i, concept := range response.UpdatedContext.CoreConcepts {
					updatedContext.CurrentTopic.KeyConcepts[i] = models.ConceptInfo{
						ConceptName: concept,
						Definition:  "LLMåˆ†æå¾—å‡ºçš„å…³é”®æ¦‚å¿µ",
						Importance:  0.8,
					}
				}
			}
		}
	}

	// æ„å»ºè¯­ä¹‰å˜æ›´
	var semanticChanges []models.SemanticChange
	for _, change := range response.SemanticChanges {
		semanticChanges = append(semanticChanges, models.SemanticChange{
			Dimension:      "topic", // é»˜è®¤ç»´åº¦
			ChangeType:     change.Type,
			NewSemantic:    change.Description,
			ChangeStrength: 0.8, // é»˜è®¤å˜åŒ–å¼ºåº¦
		})
	}

	// æ„å»ºç»“æœ
	result := &models.ContextSynthesisResult{
		UpdatedContext:   updatedContext,
		ShouldUpdate:     response.ShouldUpdate,
		UpdateConfidence: response.UpdateConfidence,
		EvaluationReason: response.EvaluationReason,
		InformationSources: models.InformationSources{
			TimelineContribution:  response.InformationSources.TimelineContribution,
			KnowledgeContribution: response.InformationSources.KnowledgeContribution,
			VectorContribution:    response.InformationSources.VectorContribution,
			ContextContribution:   response.InformationSources.ContextContribution,
		},
		SemanticChanges: semanticChanges,
	}

	// è®¾ç½®é»˜è®¤å€¼
	if result.UpdateConfidence == 0 {
		result.UpdateConfidence = 0.8
	}
	if result.EvaluationReason == "" {
		if result.ShouldUpdate {
			result.EvaluationReason = "åŸºäºæ£€ç´¢ç»“æœæ›´æ–°ä¸Šä¸‹æ–‡"
		} else {
			result.EvaluationReason = fmt.Sprintf("å½“å‰ä¸Šä¸‹æ–‡è¶³å¤Ÿï¼Œç½®ä¿¡åº¦: %.2f", result.UpdateConfidence)
		}
	}

	return result, nil
}

// createFallbackSynthesisResult åˆ›å»ºå¤‡ç”¨åˆæˆç»“æœ
func (rls *RealLLMService) createFallbackSynthesisResult(
	currentContext *models.UnifiedContextModel,
	intentAnalysis *models.IntentAnalysisResult,
) (*models.ContextSynthesisResult, error) {
	log.Printf("ğŸ”„ [çœŸå®LLM] ä½¿ç”¨å¤‡ç”¨åˆæˆæ–¹æ¡ˆ")

	shouldUpdate := currentContext == nil // å¦‚æœæ²¡æœ‰ç°æœ‰ä¸Šä¸‹æ–‡ï¼Œåˆ™éœ€è¦åˆ›å»º
	confidence := 0.7                     // å¤‡ç”¨æ–¹æ¡ˆçš„ç½®ä¿¡åº¦è¾ƒä½

	var updatedContext *models.UnifiedContextModel
	var reason string

	if shouldUpdate {
		// åˆ›å»ºåŸºç¡€ä¸Šä¸‹æ–‡
		updatedContext = &models.UnifiedContextModel{
			CreatedAt: time.Now(),
			UpdatedAt: time.Now(),
		}

		// è®¾ç½®å½“å‰ä¸»é¢˜ä¸Šä¸‹æ–‡
		updatedContext.CurrentTopic = &models.TopicContext{
			MainTopic:        fmt.Sprintf("ç”¨æˆ·åœ¨%sé¢†åŸŸè¿›è¡Œ%sç›¸å…³çš„%s", intentAnalysis.DomainContextText, intentAnalysis.CoreIntentText, intentAnalysis.ScenarioText),
			PrimaryPainPoint: fmt.Sprintf("ç”¨æˆ·æ„å›¾: %s", intentAnalysis.CoreIntentText),
			KeyConcepts: []models.ConceptInfo{
				{
					ConceptName: intentAnalysis.CoreIntentText,
					Definition:  "ç”¨æˆ·æ ¸å¿ƒæ„å›¾",
					Importance:  0.9,
				},
				{
					ConceptName: intentAnalysis.DomainContextText,
					Definition:  "æŠ€æœ¯é¢†åŸŸä¸Šä¸‹æ–‡",
					Importance:  0.8,
				},
			},
		}

		reason = "åˆå§‹åŒ–ä¸Šä¸‹æ–‡å®Œæˆ"
	} else {
		reason = fmt.Sprintf("æ— éœ€æ›´æ–°ä¸Šä¸‹æ–‡ï¼Œç½®ä¿¡åº¦: %.2f", confidence)
	}

	return &models.ContextSynthesisResult{
		UpdatedContext:   updatedContext,
		ShouldUpdate:     shouldUpdate,
		UpdateConfidence: confidence,
		EvaluationReason: reason,
		InformationSources: models.InformationSources{
			TimelineContribution:  0.25,
			KnowledgeContribution: 0.25,
			VectorContribution:    0.25,
			ContextContribution:   0.25,
		},
		SemanticChanges: []models.SemanticChange{},
	}, nil
}
