package llm

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"log"
	"net/http"
	"strings"
	"time"
)

// =============================================================================
// Ollamaæœ¬åœ°æ¨¡å‹ç»Ÿä¸€å®¢æˆ·ç«¯å®ç°
// =============================================================================

// OllamaLocalClient Ollamaæœ¬åœ°æ¨¡å‹ç»Ÿä¸€é€‚é…å™¨
type OllamaLocalClient struct {
	*BaseAdapter
	baseURL     string
	modelName   string
	displayName string
}

// OllamaRequest Ollamaè¯·æ±‚æ ¼å¼
type OllamaRequest struct {
	Model     string                 `json:"model"`
	Prompt    string                 `json:"prompt"`
	Stream    bool                   `json:"stream"`
	Context   []int                  `json:"context,omitempty"`
	KeepAlive string                 `json:"keep_alive,omitempty"`
	Options   map[string]interface{} `json:"options,omitempty"`
}

// OllamaResponse Ollamaå“åº”æ ¼å¼
type OllamaResponse struct {
	Model              string `json:"model"`
	Response           string `json:"response"`
	Done               bool   `json:"done"`
	Context            []int  `json:"context,omitempty"`
	TotalDuration      int64  `json:"total_duration"`
	LoadDuration       int64  `json:"load_duration"`
	PromptEvalCount    int    `json:"prompt_eval_count"`
	PromptEvalDuration int64  `json:"prompt_eval_duration"`
	EvalCount          int    `json:"eval_count"`
	EvalDuration       int64  `json:"eval_duration"`
}

// OllamaErrorResponse Ollamaé”™è¯¯å“åº”
type OllamaErrorResponse struct {
	Error string `json:"error"`
}

// NewOllamaLocalClient åˆ›å»ºOllamaæœ¬åœ°å®¢æˆ·ç«¯
func NewOllamaLocalClient(config *LLMConfig) (LLMClient, error) {
	baseURL := config.BaseURL
	if baseURL == "" {
		baseURL = "http://localhost:11434"
	}

	modelName := config.Model
	if modelName == "" {
		modelName = "codeqwen:7b" // é»˜è®¤æ¨¡å‹
	}

	// ç”Ÿæˆå‹å¥½çš„æ˜¾ç¤ºåç§°
	displayName := generateDisplayName(modelName)

	client := &OllamaLocalClient{
		BaseAdapter: NewBaseAdapter(ProviderOllamaLocal, config),
		baseURL:     baseURL,
		modelName:   modelName,
		displayName: displayName,
	}

	// è®¾ç½®èƒ½åŠ›ï¼ˆæ ¹æ®æ¨¡å‹ç±»å‹åŠ¨æ€è°ƒæ•´ï¼‰
	capabilities := getModelCapabilities(modelName)
	client.SetCapabilities(capabilities)

	return client, nil
}

// generateDisplayName ç”Ÿæˆå‹å¥½çš„æ˜¾ç¤ºåç§°
func generateDisplayName(modelName string) string {
	switch {
	case strings.Contains(modelName, "codeqwen"):
		return "CodeQwen-Local"
	case strings.Contains(modelName, "deepseek-coder"):
		if strings.Contains(modelName, "33b") {
			return "DeepSeekCoder-33B-Local"
		} else if strings.Contains(modelName, "16b") {
			return "DeepSeekCoder-16B-Local"
		} else if strings.Contains(modelName, "6.7b") {
			return "DeepSeekCoder-6.7B-Local"
		}
		return "DeepSeekCoder-Local"
	default:
		return fmt.Sprintf("Ollama-%s", modelName)
	}
}

// getModelCapabilities æ ¹æ®æ¨¡å‹è·å–èƒ½åŠ›é…ç½®
func getModelCapabilities(modelName string) *LLMCapabilities {
	baseCapabilities := &LLMCapabilities{
		SupportedFormats:  []string{"text", "json", "code"},
		SupportsStreaming: true,
		SupportsBatch:     false,
		CostPerToken:      0.0, // æœ¬åœ°æ¨¡å‹å…è´¹
		LatencyMs:         200, // æœ¬åœ°å“åº”æ›´å¿«
	}

	// æ ¹æ®æ¨¡å‹è®¾ç½®ç‰¹å®šå‚æ•°
	switch {
	case strings.Contains(modelName, "codeqwen"):
		baseCapabilities.MaxTokens = 8192
		baseCapabilities.Models = []string{"codeqwen:7b", "codeqwen:latest"}

	case strings.Contains(modelName, "deepseek-coder"):
		if strings.Contains(modelName, "33b") {
			baseCapabilities.MaxTokens = 16384
			baseCapabilities.LatencyMs = 800 // å¤§æ¨¡å‹ç¨æ…¢
		} else if strings.Contains(modelName, "16b") {
			baseCapabilities.MaxTokens = 16384
			baseCapabilities.LatencyMs = 500
		} else {
			baseCapabilities.MaxTokens = 8192
			baseCapabilities.LatencyMs = 300
		}
		baseCapabilities.Models = []string{
			"deepseek-coder:6.7b", "deepseek-coder:6.7b-instruct",
			"deepseek-coder:33b", "deepseek-coder:33b-instruct",
			"deepseek-coder-v2:16b",
		}

	default:
		baseCapabilities.MaxTokens = 4096
		baseCapabilities.Models = []string{modelName}
	}

	return baseCapabilities
}

// Complete å®Œæˆå¯¹è¯
func (oc *OllamaLocalClient) Complete(ctx context.Context, req *LLMRequest) (*LLMResponse, error) {
	startTime := time.Now()

	fmt.Printf("ğŸ¤– [%s] å¼€å§‹å¤„ç†æœ¬åœ°æ¨¡å‹è¯·æ±‚...\n", oc.displayName)
	fmt.Printf("ğŸ“ è¯·æ±‚å‚æ•°: MaxTokens=%d, Temperature=%.1f, Format=%s\n",
		req.MaxTokens, req.Temperature, req.Format)
	fmt.Printf("ğŸ“‹ ç³»ç»Ÿæç¤ºè¯é•¿åº¦: %d å­—ç¬¦\n", len(req.SystemPrompt))
	fmt.Printf("ğŸ“‹ ç”¨æˆ·æç¤ºè¯é•¿åº¦: %d å­—ç¬¦\n", len(req.Prompt))

	// 1. æ£€æŸ¥é™æµï¼ˆæœ¬åœ°æ¨¡å‹é€šå¸¸ä¸éœ€è¦é™æµï¼‰
	fmt.Printf("ğŸš¦ [æ­¥éª¤1] æ£€æŸ¥é™æµ...\n")

	// ğŸ”¥ æœ¬åœ°æ¨¡å‹è·³è¿‡é™æµæ£€æŸ¥ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
	skipRateLimit := true
	if req.Metadata != nil {
		if skip, exists := req.Metadata["skip_rate_limit"]; exists {
			if skipBool, ok := skip.(bool); ok {
				skipRateLimit = skipBool
			}
		}
	}

	if !skipRateLimit {
		if err := oc.CheckRateLimit(ctx); err != nil {
			fmt.Printf("âŒ é™æµæ£€æŸ¥å¤±è´¥: %v\n", err)
			return nil, err
		}
		fmt.Printf("âœ… é™æµæ£€æŸ¥é€šè¿‡\n")
	} else {
		fmt.Printf("âœ… é™æµæ£€æŸ¥è·³è¿‡ï¼ˆæœ¬åœ°æ¨¡å‹ï¼‰\n")
	}

	// 2. æ£€æŸ¥ç†”æ–­å™¨
	fmt.Printf("ğŸ”Œ [æ­¥éª¤2] æ£€æŸ¥ç†”æ–­å™¨...\n")
	if err := oc.CheckCircuitBreaker(); err != nil {
		fmt.Printf("âŒ ç†”æ–­å™¨æ£€æŸ¥å¤±è´¥: %v\n", err)
		return nil, err
	}
	fmt.Printf("âœ… ç†”æ–­å™¨æ£€æŸ¥é€šè¿‡\n")

	// 3. è½¬æ¢è¯·æ±‚æ ¼å¼
	fmt.Printf("ğŸ”„ [æ­¥éª¤3] è½¬æ¢è¯·æ±‚æ ¼å¼...\n")
	ollamaReq := oc.convertToOllamaFormat(req)
	fmt.Printf("âœ… è¯·æ±‚æ ¼å¼è½¬æ¢å®Œæˆ: Model=%s\n", ollamaReq.Model)

	// 4. å‘é€è¯·æ±‚åˆ°Ollama
	fmt.Printf("ğŸ“¡ [æ­¥éª¤4] å‘é€è¯·æ±‚åˆ°Ollama (%s)...\n", oc.baseURL)
	resp, err := oc.sendRequest(ctx, ollamaReq)
	if err != nil {
		fmt.Printf("âŒ Ollamaè¯·æ±‚å¤±è´¥: %v\n", err)
		oc.RecordFailure()
		return nil, err
	}
	fmt.Printf("âœ… Ollamaè¯·æ±‚æˆåŠŸ\n")

	// 5. è½¬æ¢å“åº”æ ¼å¼
	fmt.Printf("ğŸ”„ [æ­¥éª¤5] è½¬æ¢å“åº”æ ¼å¼...\n")
	oc.RecordSuccess()
	result := oc.convertFromOllamaFormat(resp, time.Since(startTime))

	fmt.Printf("âœ… %så¤„ç†å®Œæˆ\n", oc.displayName)
	fmt.Printf("ğŸ“Š å“åº”ç»Ÿè®¡: TokensUsed=%d, Duration=%v\n",
		result.TokensUsed, result.Duration)
	fmt.Printf("ğŸ“„ å“åº”å†…å®¹é•¿åº¦: %d å­—ç¬¦\n", len(result.Content))

	return result, nil
}

// BatchComplete æ‰¹é‡å®Œæˆ
func (oc *OllamaLocalClient) BatchComplete(ctx context.Context, reqs []*LLMRequest) ([]*LLMResponse, error) {
	responses := make([]*LLMResponse, len(reqs))

	for i, req := range reqs {
		resp, err := oc.Complete(ctx, req)
		if err != nil {
			return nil, fmt.Errorf("batch request %d failed: %w", i, err)
		}
		responses[i] = resp
	}

	return responses, nil
}

// StreamComplete æµå¼å®Œæˆ
func (oc *OllamaLocalClient) StreamComplete(ctx context.Context, req *LLMRequest) (<-chan *LLMStreamResponse, error) {
	ch := make(chan *LLMStreamResponse, 1)

	go func() {
		defer close(ch)

		resp, err := oc.Complete(ctx, req)
		if err != nil {
			ch <- &LLMStreamResponse{
				Error:    err,
				Provider: ProviderOllamaLocal,
			}
			return
		}

		ch <- &LLMStreamResponse{
			Content:  resp.Content,
			Done:     true,
			Provider: ProviderOllamaLocal,
		}
	}()

	return ch, nil
}

// HealthCheck å¥åº·æ£€æŸ¥
func (oc *OllamaLocalClient) HealthCheck(ctx context.Context) error {
	// æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦å¯ç”¨
	healthURL := oc.baseURL + "/api/tags"

	httpReq, err := http.NewRequestWithContext(ctx, "GET", healthURL, nil)
	if err != nil {
		return fmt.Errorf("create health check request failed: %w", err)
	}

	httpResp, err := oc.httpClient.Do(httpReq)
	if err != nil {
		return fmt.Errorf("ollama service not available: %w", err)
	}
	defer httpResp.Body.Close()

	if httpResp.StatusCode != http.StatusOK {
		return fmt.Errorf("ollama service unhealthy: HTTP %d", httpResp.StatusCode)
	}

	return nil
}

// GetModel è·å–æ¨¡å‹åç§°
func (oc *OllamaLocalClient) GetModel() string {
	return oc.modelName
}

// convertToOllamaFormat è½¬æ¢ä¸ºOllamaæ ¼å¼
func (oc *OllamaLocalClient) convertToOllamaFormat(req *LLMRequest) *OllamaRequest {
	// æ„å»ºå®Œæ•´çš„prompt
	fullPrompt := ""
	if req.SystemPrompt != "" {
		fullPrompt = fmt.Sprintf("System: %s\n\nUser: %s", req.SystemPrompt, req.Prompt)
	} else {
		fullPrompt = req.Prompt
	}

	// è®¾ç½®é€‰é¡¹
	options := make(map[string]interface{})
	if req.MaxTokens > 0 {
		options["num_predict"] = req.MaxTokens
	}
	if req.Temperature >= 0 {
		options["temperature"] = req.Temperature
	}

	// ä½¿ç”¨é…ç½®ä¸­æŒ‡å®šçš„æ¨¡å‹ï¼Œæˆ–è¯·æ±‚ä¸­æŒ‡å®šçš„æ¨¡å‹
	modelName := oc.modelName
	if req.Model != "" {
		modelName = req.Model
	}

	return &OllamaRequest{
		Model:     modelName,
		Prompt:    fullPrompt,
		Stream:    false,
		Options:   options,
		Context:   nil,  // ğŸ”¥ å¼ºåˆ¶æ¸…é™¤ä¸Šä¸‹æ–‡ç¼“å­˜ï¼Œé¿å…å†å²å¯¹è¯æ±¡æŸ“
		KeepAlive: "0s", // ğŸ”¥ ç«‹å³å¸è½½æ¨¡å‹ï¼Œç¡®ä¿æ¯æ¬¡è¯·æ±‚å®Œå…¨ç‹¬ç«‹
	}
}

// convertFromOllamaFormat è½¬æ¢Ollamaå“åº”æ ¼å¼
func (oc *OllamaLocalClient) convertFromOllamaFormat(resp *OllamaResponse, duration time.Duration) *LLMResponse {
	// ä¼°ç®—tokenä½¿ç”¨é‡ï¼ˆç®€å•ä¼°ç®—ï¼šå­—ç¬¦æ•°/4ï¼‰
	tokensUsed := (len(resp.Response) + len(resp.Model)) / 4

	return &LLMResponse{
		Content:    resp.Response,
		TokensUsed: tokensUsed,
		Model:      resp.Model,
		Provider:   ProviderOllamaLocal,
		Duration:   duration,
		Metadata: map[string]interface{}{
			"display_name":         oc.displayName,
			"total_duration":       resp.TotalDuration,
			"load_duration":        resp.LoadDuration,
			"prompt_eval_count":    resp.PromptEvalCount,
			"prompt_eval_duration": resp.PromptEvalDuration,
			"eval_count":           resp.EvalCount,
			"eval_duration":        resp.EvalDuration,
			"tokens_per_second":    calculateTokensPerSecond(resp.EvalCount, resp.EvalDuration),
		},
	}
}

// calculateTokensPerSecond è®¡ç®—æ¯ç§’tokenæ•°
func calculateTokensPerSecond(evalCount int, evalDuration int64) float64 {
	if evalDuration == 0 {
		return 0
	}
	// evalDuration is in nanoseconds
	seconds := float64(evalDuration) / 1e9
	return float64(evalCount) / seconds
}

// sendRequest å‘é€HTTPè¯·æ±‚åˆ°Ollama
func (oc *OllamaLocalClient) sendRequest(ctx context.Context, req *OllamaRequest) (*OllamaResponse, error) {
	// åºåˆ—åŒ–è¯·æ±‚
	reqBody, err := json.Marshal(req)
	if err != nil {
		return nil, fmt.Errorf("marshal request failed: %w", err)
	}

	// ğŸ”¥ æ‰“å°å…¥å‚
	log.Printf("ğŸ“¡ [Ollamaå…¥å‚] %s", string(reqBody))

	// åˆ›å»ºHTTPè¯·æ±‚
	httpReq, err := http.NewRequestWithContext(ctx, "POST", oc.baseURL+"/api/generate", bytes.NewBuffer(reqBody))
	if err != nil {
		return nil, fmt.Errorf("create request failed: %w", err)
	}

	// è®¾ç½®è¯·æ±‚å¤´
	httpReq.Header.Set("Content-Type", "application/json")

	// å‘é€è¯·æ±‚
	httpResp, err := oc.httpClient.Do(httpReq)
	if err != nil {
		return nil, fmt.Errorf("send request failed: %w", err)
	}
	defer httpResp.Body.Close()

	// è¯»å–å“åº”
	respBody, err := io.ReadAll(httpResp.Body)
	if err != nil {
		return nil, fmt.Errorf("read response failed: %w", err)
	}

	// ğŸ”¥ æ‰“å°å‡ºå‚
	log.Printf("ğŸ“¨ [Ollamaå‡ºå‚] %s", string(respBody))

	// æ£€æŸ¥HTTPçŠ¶æ€ç 
	if httpResp.StatusCode != http.StatusOK {
		var errorResp OllamaErrorResponse
		if err := json.Unmarshal(respBody, &errorResp); err == nil {
			return nil, &LLMError{
				Provider:  ProviderOllamaLocal,
				Code:      "OLLAMA_ERROR",
				Message:   errorResp.Error,
				Retryable: httpResp.StatusCode >= 500,
			}
		}
		return nil, fmt.Errorf("HTTP %d: %s", httpResp.StatusCode, string(respBody))
	}

	// è§£æå“åº”
	var resp OllamaResponse
	if err := json.Unmarshal(respBody, &resp); err != nil {
		return nil, fmt.Errorf("unmarshal response failed: %w", err)
	}

	return &resp, nil
}
